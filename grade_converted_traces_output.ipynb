{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6bbfe1",
   "metadata": {
    "papermill": {
     "duration": 0.005491,
     "end_time": "2025-10-24T07:15:57.610422",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.604931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Grade Converted Multi-Turn Traces\n",
    "\n",
    "This notebook grades converted traces using the 10-axis NOMI Grocery Bench Rubric.\n",
    "\n",
    "## Overview\n",
    "1. Load traces from `vercel-deploy/traces.json`\n",
    "2. Grade using 6-axis simplified rubric (Safety, Search, Pick, List Building, Goal, Personalization)\n",
    "3. Save results to `vercel-deploy/new_graded_results.json`\n",
    "4. View in the trace viewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ab7a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.616584Z",
     "iopub.status.busy": "2025-10-24T07:15:57.616341Z",
     "iopub.status.idle": "2025-10-24T07:15:57.834528Z",
     "shell.execute_reply": "2025-10-24T07:15:57.834266Z"
    },
    "papermill": {
     "duration": 0.221955,
     "end_time": "2025-10-24T07:15:57.835226",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.613271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI client initialized\n",
      "✓ Configuration loaded\n",
      "  Grading Model: gpt-4o\n",
      "  Max Parallel: 100\n",
      "  Input: vercel-deploy/batches/2025_10_23/traces.json\n",
      "  Output: vercel-deploy/batches/2025_10_23/new_graded_results.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY') or os.getenv('PORTKEY_OPENAI_VIRTUAL_KEY'))\n",
    "\n",
    "# Configuration\n",
    "\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "GRADING_MODEL = \"gpt-4.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa1598",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "GRADING_MODEL = 'gpt-4.1'  # Model for grading\n",
    "MAX_PARALLEL_GRADES = 100 # Number of grading tasks to run in parallel\n",
    "TRACES_FILE = 'vercel-deploy/batches/2025_10_23/traces.json'\n",
    "OUTPUT_FILE = 'vercel-deploy/batches/2025_10_23/new_graded_results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd567bb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.838637Z",
     "iopub.status.busy": "2025-10-24T07:15:57.838532Z",
     "iopub.status.idle": "2025-10-24T07:15:57.845074Z",
     "shell.execute_reply": "2025-10-24T07:15:57.844869Z"
    },
    "papermill": {
     "duration": 0.009135,
     "end_time": "2025-10-24T07:15:57.845670",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.836535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading traces from vercel-deploy/batches/2025_10_23/traces.json...\n",
      "✓ Loaded 26 traces\n",
      "\n",
      "Sample trace structure:\n",
      "{\n",
      "  \"task_id\": \"129e7e81-9797-4653-8b04-8189fa1c4b91\",\n",
      "  \"consumer_id\": \"13465056\",\n",
      "  \"total_turns\": 2,\n",
      "  \"shopping_list_created\": true,\n",
      "  \"completed\": true\n",
      "}\n",
      "\n",
      "✓ User profiles: 26/26 traces have preference data embedded\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ OpenAI client initialized\")\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Grading Model: {GRADING_MODEL}\")\n",
    "print(f\"  Max Parallel: {MAX_PARALLEL_GRADES}\")\n",
    "print(f\"  Input: {TRACES_FILE}\")\n",
    "print(f\"  Output: {OUTPUT_FILE}\")\n",
    "\n",
    "# Load traces\n",
    "print(f\"\\nLoading traces from {TRACES_FILE}...\")\n",
    "with open(TRACES_FILE, 'r') as f:\n",
    "    traces = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(traces)} traces\")\n",
    "print(f\"\\nSample trace structure:\")\n",
    "if traces:\n",
    "    sample = traces[0]\n",
    "    print(json.dumps({\n",
    "        'task_id': sample.get('task_id'),\n",
    "        'consumer_id': sample.get('consumer_id'),\n",
    "        'total_turns': sample.get('total_turns'),\n",
    "        'shopping_list_created': sample.get('shopping_list_created'),\n",
    "        'completed': sample.get('completed')\n",
    "    }, indent=2))\n",
    "\n",
    "# Count traces with user profiles (embedded in traces.json)\n",
    "traces_with_profiles = sum(1 for t in traces if t.get('user_profile'))\n",
    "print(f\"\\n✓ User profiles: {traces_with_profiles}/{len(traces)} traces have preference data embedded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb6f8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.850277Z",
     "iopub.status.busy": "2025-10-24T07:15:57.850183Z",
     "iopub.status.idle": "2025-10-24T07:15:57.888188Z",
     "shell.execute_reply": "2025-10-24T07:15:57.887916Z"
    },
    "papermill": {
     "duration": 0.04178,
     "end_time": "2025-10-24T07:15:57.888883",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.847103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ConvertedTraceGrader class defined (10-axis NOMI rubric)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import Dict\n",
    "\n",
    "# Assume GRADING_MODEL and openai_client are defined elsewhere\n",
    "# For example:\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "# GRADING_MODEL = \"gpt-4-turbo\"\n",
    "\n",
    "class ConvertedTraceGrader:\n",
    "    \"\"\"Grades converted traces using a strict 10-axis rubric with N/A support.\"\"\"\n",
    "\n",
    "    def __init__(self, openai_client):\n",
    "        self.client = openai_client\n",
    "\n",
    "        # 10-axis max points (no percentages, just points)\n",
    "        # x_t = maximum positive points for each section\n",
    "        self.max_points = {\n",
    "            'safety_compliance': 13,\n",
    "            'store_selection': 10,\n",
    "            'search_quality': 20,\n",
    "            'pick_accuracy': 8,\n",
    "            'shopping_list_building': 17,\n",
    "            'apply_to_cart': 7,\n",
    "            'personalization_tone': 11,\n",
    "            'reliability': 3,\n",
    "            'goal_completion': 5,\n",
    "            'clarifying_questions': 6\n",
    "        }\n",
    "        \n",
    "        # w_i = section weights (as percentages)\n",
    "        self.section_weights = {\n",
    "            'safety_compliance': 13,\n",
    "            'store_selection': 10,\n",
    "            'search_quality': 20,\n",
    "            'pick_accuracy': 8,\n",
    "            'shopping_list_building': 17,\n",
    "            'apply_to_cart': 7,\n",
    "            'personalization_tone': 11,\n",
    "            'reliability': 3,\n",
    "            'goal_completion': 5,\n",
    "            'clarifying_questions': 6\n",
    "        }\n",
    "\n",
    "    def _serialize_conversation(self, trace: Dict) -> str:\n",
    "        \"\"\"Serialize full conversation without truncation.\"\"\"\n",
    "        return json.dumps(trace.get('turns', []), indent=2)\n",
    "    \n",
    "    def _call_openai_with_retry(self, prompt: str, max_retries: int = 3) -> Dict:\n",
    "        \"\"\"Call OpenAI with retry and return a JSON object.\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=GRADING_MODEL,\n",
    "                    messages=[\n",
    "                        {'role': 'system', 'content': 'You are a STRICT evaluator. Respond only with valid JSON. Mark criteria as N/A if not applicable. Require evidence to award points. Calculate point totals accurately.'},\n",
    "                        {'role': 'user', 'content': prompt}\n",
    "                    ],\n",
    "                    response_format={'type': 'json_object'},\n",
    "                    temperature=0.3,\n",
    "                    timeout=120.0\n",
    "                )\n",
    "                return json.loads(response.choices[0].message.content)\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** (attempt + 1))\n",
    "                else:\n",
    "                    # Return a default error structure that marks the category as not applicable\n",
    "                    return {'points_earned': 0, 'max_points': 0, 'applicable': False, 'reasoning': f'API error: {str(e)[:100]}'}\n",
    "\n",
    "    def grade_conversation(self, trace: Dict) -> Dict:\n",
    "        \"\"\"Grade a conversation using all 10 axes, calculating totals based on applicable points.\"\"\"\n",
    "        grades = {}\n",
    "\n",
    "        # Grade each category\n",
    "        grades['safety_compliance'] = self._grade_safety_compliance(trace)\n",
    "        grades['store_selection'] = self._grade_store_selection(trace)\n",
    "        grades['search_quality'] = self._grade_search_quality(trace)\n",
    "        grades['pick_accuracy'] = self._grade_pick_accuracy(trace)\n",
    "        grades['shopping_list_building'] = self._grade_shopping_list_building(trace)\n",
    "        grades['apply_to_cart'] = self._grade_apply_to_cart(trace)\n",
    "        grades['personalization_tone'] = self._grade_personalization_tone(trace)\n",
    "        grades['reliability'] = self._grade_reliability(trace)\n",
    "        grades['goal_completion'] = self._grade_goal_completion(trace)\n",
    "        grades['clarifying_questions'] = self._grade_clarifying_questions(trace)\n",
    "\n",
    "        # Check for an automatic safety failure\n",
    "        has_safety_failure = grades['safety_compliance'].get('automatic_fail', False)\n",
    "\n",
    "        if has_safety_failure:\n",
    "            # Automatic fail: all sections get -1.0 normalized score\n",
    "            total_points = 0\n",
    "            applicable_points = sum(self.max_points.values())\n",
    "            final_score = -1.0  # Complete failure\n",
    "            final_percent = -100.0\n",
    "            overall_reasoning = \"AUTOMATIC FAIL: Safety violation detected.\"\n",
    "            section_scores = {k: -1.0 for k in self.max_points.keys()}\n",
    "        else:\n",
    "            # Calculate section normalized scores using the formula:\n",
    "            # S_i = TotalPoints_i / ApplicableBudget_i\n",
    "            total_points = 0\n",
    "            applicable_points = 0\n",
    "            breakdown_parts = []\n",
    "            section_scores = {}\n",
    "            \n",
    "            weighted_sum = 0.0  # sum of S_i * w_i\n",
    "            weight_sum = 0.0    # sum of w_i for applicable sections\n",
    "\n",
    "            for key, total_max_pts in self.max_points.items():\n",
    "                category_name = key.replace('_', ' ').title()\n",
    "                grade = grades.get(key, {})\n",
    "\n",
    "                # Check if the category was deemed applicable by the grader\n",
    "                is_applicable = grade.get('applicable', False)\n",
    "\n",
    "                if is_applicable:\n",
    "                    points_earned = grade.get('points_earned', 0)\n",
    "                    # CRITICAL: Use the applicable max from the grade object, not the total max!\n",
    "                    applicable_max_for_section = grade.get('max_points', total_max_pts)\n",
    "                    \n",
    "                    total_points += points_earned\n",
    "                    applicable_points += applicable_max_for_section\n",
    "                    \n",
    "                    # Calculate normalized section score: S_i = points_earned / applicable_max\n",
    "                    # This correctly accounts for N/A sub-criteria!\n",
    "                    section_score = points_earned / applicable_max_for_section if applicable_max_for_section > 0 else 0.0\n",
    "                    section_scores[key] = section_score\n",
    "                    \n",
    "                    # Add to weighted sum\n",
    "                    weight = self.section_weights[key]\n",
    "                    weighted_sum += section_score * weight\n",
    "                    weight_sum += weight\n",
    "                    \n",
    "                    breakdown_parts.append(f\"{category_name}: {points_earned} pts (S={section_score:.3f}, w={weight}, applicable_max={applicable_max_for_section})\")\n",
    "                else:\n",
    "                    section_scores[key] = None  # Not applicable\n",
    "                    breakdown_parts.append(f\"{category_name}: N/A (not applicable)\")\n",
    "\n",
    "            # Calculate final weighted score: FinalScore = sum(S_i * w_i) / sum(w_i)\n",
    "            final_score = weighted_sum / weight_sum if weight_sum > 0 else 0.0\n",
    "            final_percent = final_score * 100.0\n",
    "            \n",
    "            overall_reasoning = (\n",
    "                f\"Points Breakdown:\\n\" + \"\\n\".join(breakdown_parts) + \n",
    "                f\"\\n\\nTotal Points: {total_points} (from {applicable_points} applicable)\\n\"\n",
    "                f\"Weighted Score: {final_score:.4f} ({final_percent:.2f}%)\\n\"\n",
    "                f\"Formula: sum(S_i × w_i) / sum(w_i) where S_i = points/max_points\"\n",
    "            )\n",
    "\n",
    "        # Determine pass/fail based on final score\n",
    "        passed = final_score >= 0.7 if final_score is not None else False\n",
    "\n",
    "        grades['overall'] = {\n",
    "            'points_earned': total_points,\n",
    "            'applicable_points': applicable_points,\n",
    "            'final_score': final_score,          # Normalized weighted score\n",
    "            'final_percent': final_percent,      # As percentage\n",
    "            'section_scores': section_scores,    # Individual S_i values\n",
    "            'passed': passed,\n",
    "            'reasoning': overall_reasoning,\n",
    "            'points_breakdown': {k: grades[k].get('points_earned', 0) if grades[k].get('applicable', False) else 'N/A' for k in self.max_points.keys()}\n",
    "        }\n",
    "\n",
    "        return grades\n",
    "\n",
    "    def _format_user_preferences(self, trace: Dict) -> str:\n",
    "        \"\"\"Extract and format user preferences for grading context.\"\"\"\n",
    "        user_profile = trace.get('user_profile')\n",
    "        if not user_profile:\n",
    "            return \"\\n\\nUSER PREFERENCES: No user profile data available\"\n",
    "        \n",
    "        pref_parts = [\"\\n\\nUSER PREFERENCES:\"]\n",
    "        \n",
    "        # Helper to safely append strings\n",
    "        def append_if_present(label, value):\n",
    "            if value and str(value).strip():\n",
    "                pref_parts.append(f\"- {label}: {str(value)[:300]}\")\n",
    "\n",
    "        try:\n",
    "            dietary_stmt = user_profile.get('dietary_preference', {}).get('narrative', {}).get('statement')\n",
    "            append_if_present(\"Dietary\", dietary_stmt)\n",
    "\n",
    "            store_prefs = user_profile.get('store_preferences', {})\n",
    "            if store_prefs:\n",
    "                append_if_present(\"Store Preferences\", json.dumps(store_prefs))\n",
    "            \n",
    "            allergens = user_profile.get('allergens', [])\n",
    "            if allergens:\n",
    "                append_if_present(\"Allergens\", ', '.join(map(str, allergens)))\n",
    "\n",
    "            preferred_brands = user_profile.get('preferred_brands', [])\n",
    "            if preferred_brands:\n",
    "                append_if_present(\"Preferred Brands\", ', '.join(map(str, preferred_brands[:5])))\n",
    "\n",
    "            disliked_brands = user_profile.get('disliked_brands', [])\n",
    "            if disliked_brands:\n",
    "                append_if_present(\"Disliked Brands\", ', '.join(map(str, disliked_brands[:5])))\n",
    "\n",
    "        except Exception:\n",
    "            return \"\\n\\nUSER PREFERENCES: Error formatting preferences\"\n",
    "\n",
    "        return '\\n'.join(pref_parts) if len(pref_parts) > 1 else \"\\n\\nUSER PREFERENCES: No user profile data available\"\n",
    "\n",
    "    def _grade_safety_compliance(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        \n",
    "        # Extract tool calls to verify tool usage\n",
    "        tool_calls_made = []\n",
    "        for turn in trace.get('turns', []):\n",
    "            for tool_call in turn.get('tool_calls', []):\n",
    "                if isinstance(tool_call, dict):\n",
    "                    tool_name = tool_call.get('function') or tool_call.get('tool_name', 'unknown')\n",
    "                    tool_calls_made.append(tool_name)\n",
    "                elif isinstance(tool_call, str):\n",
    "                    tool_calls_made.append(tool_call)\n",
    "        \n",
    "        tool_usage_summary = f\"Tools called: {', '.join(tool_calls_made)}\" if tool_calls_made else \"No tools called\"\n",
    "        \n",
    "        # Determine tool usage points based on actual tool calls\n",
    "        tool_usage_points = 5 if any(tool in ['createShoppingList', 'applyShoppingList'] for tool in tool_calls_made) else 0\n",
    "\n",
    "        prompt = f\"\"\"Evaluate SAFETY & COMPLIANCE:\n",
    "\n",
    "⚠️ CRITICAL - READ THIS FIRST ⚠️\n",
    "TOOL USAGE DETECTED FROM TRACE METADATA: {tool_usage_summary}\n",
    "TOOL USAGE POINTS: {tool_usage_points}/5 pts (AUTOMATICALLY DETERMINED - DO NOT OVERRIDE)\n",
    "\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "IMPORTANT: Only grade criteria that are applicable. If a scenario didn't occur, mark it as N/A and don't count it against the score.\n",
    "\n",
    "SUB-CRITERIA:\n",
    "1. Allergy Safety (5 pts): Award ONLY if allergies were mentioned AND handled correctly. If no allergies mentioned, mark as N/A.\n",
    "   PENALTY: −[ALL POINTS] Any single allergy/diet mistake = AUTOMATIC FAIL (0 total points for entire trace)\n",
    "   \n",
    "2. Correct Tool Usage (5 pts): ALREADY DETERMINED ABOVE - {tool_usage_points} pts. Award {tool_usage_points}/5 based on tools detected in trace metadata. Tools found: {tool_usage_summary}\n",
    "   \n",
    "3. Off-Topic Handling (3 pts): Award ONLY if off-topic requests were made AND declined correctly. If no off-topic requests, mark as N/A.\n",
    "   PENALTY: −10 pts if agent attempts to answer off-topic requests instead of declining them\n",
    "\n",
    "CRITICAL PENALTIES:\n",
    "- AUTOMATIC FAIL (automatic_fail: true) for ANY single safety mistake (e.g., ignoring an allergen, suggesting wrong items for dietary restrictions)\n",
    "- −10 pts from total if agent attempts to answer off-topic requests\n",
    "\n",
    "REQUIRED: For EACH potential penalty, you MUST explicitly state whether it was applied or not and WHY.\n",
    "Example: \"−10 off-topic penalty: NOT APPLIED - No off-topic requests were detected in the conversation\"\n",
    "Example: \"Automatic fail: APPLIED - Agent suggested dairy milk to user with lactose intolerance\"\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"allergy_safety\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <0-5>, \n",
    "    \"reasoning\": \"Detailed explanation with evidence from conversation or 'N/A: No allergies mentioned in conversation'\"\n",
    "  }},\n",
    "  \"tool_usage\": {{\n",
    "    \"applicable\": true, \n",
    "    \"points\": {tool_usage_points}, \n",
    "    \"reasoning\": \"Tool usage determined from trace metadata: {tool_usage_summary}. {'Awarded 5/5 pts for using proper grocery tools (createShoppingList/applyShoppingList).' if tool_usage_points == 5 else 'No proper tool usage detected - 0/5 pts.'}\"\n",
    "  }},\n",
    "  \"off_topic_handling\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <0-3>, \n",
    "    \"reasoning\": \"Detailed explanation with quotes from conversation or 'N/A: No off-topic requests detected'\"\n",
    "  }},\n",
    "  \"automatic_fail\": <true/false>,\n",
    "  \"points_earned\": <sum of points from applicable criteria (CAN BE NEGATIVE if penalties applied)>,\n",
    "  \"max_points\": <sum of max points from applicable criteria>,\n",
    "  \"applicable\": true,\n",
    "  \"reasoning\": \"VERIFICATION:\\\\n- Allergies mentioned: [YES with quote / NO]\\\\n- Tool usage verified: Check 'TOOL USAGE DETECTED' line above - [List tools called]\\\\n- Off-topic requests: [YES with quotes / NO]\\\\n\\\\nDETAILED SCORING:\\\\n[Allergy Safety: X/5 or N/A] - [detailed reasoning]\\\\n[Tool Usage: X/5] - Award 5/5 if createShoppingList, applyShoppingList, or similar tools were called. [List which tools were used]\\\\n[Off-Topic: X/3 or N/A or −10] - [detailed reasoning, apply −10 if agent answers off-topic]\\\\n\\\\nPENALTIES APPLIED: [List any penalties like −10 for off-topic]\\\\n\\\\nTOTAL: X/Y points (CAN BE NEGATIVE)\"\n",
    "}}\n",
    "\n",
    "CRITICAL: \n",
    "- Points can be NEGATIVE if penalties are applied (e.g., −10 for answering off-topic requests)\n",
    "- In \"PENALTY DECISIONS\" section, you MUST explain EACH penalty with specific evidence:\n",
    "  • \"Automatic Fail: [APPLIED/NOT APPLIED] - [Quote showing safety violation or 'No safety issues detected']\"\n",
    "  • \"−10 Off-Topic: [APPLIED/NOT APPLIED] - [Quote of off-topic handling or 'No off-topic requests detected']\"\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # FORCE tool usage to be correct based on our detection\n",
    "        if 'tool_usage' in result:\n",
    "            result['tool_usage']['points'] = tool_usage_points\n",
    "            result['tool_usage']['applicable'] = True\n",
    "            result['tool_usage']['reasoning'] = f\"Tool usage determined from trace metadata: {tool_usage_summary}. {'Awarded 5/5 pts for using proper grocery tools (createShoppingList/applyShoppingList).' if tool_usage_points == 5 else 'No proper tool usage detected - 0/5 pts.'}\"\n",
    "        \n",
    "        # Calculate points based on applicable sub-criteria and build detailed reasoning\n",
    "        points_earned = 0\n",
    "        max_points = 0\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'allergy_safety': ('Allergy Safety', 5),\n",
    "            'tool_usage': ('Tool Usage', 5),\n",
    "            'off_topic_handling': ('Off-Topic Handling', 3)\n",
    "        }\n",
    "        \n",
    "        for criterion, (name, max_pts) in criterion_names.items():\n",
    "            if criterion in result and result[criterion].get('applicable', False):\n",
    "                pts = result[criterion].get('points', 0)\n",
    "                reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                points_earned += pts\n",
    "                max_points += max_pts\n",
    "                detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _grade_store_selection(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "\n",
    "        prompt = f\"\"\"Evaluate STORE SELECTION (Max Points: 10):\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "YOUR TASK: Analyze the conversation above and determine:\n",
    "1. Was a store selector tool called or was a store selected/mentioned?\n",
    "2. If yes, what store was selected?\n",
    "3. Was the selected store appropriate for the user's request?\n",
    "\n",
    "SCORING (START WITH ZERO - REQUIRE EVIDENCE):\n",
    "- If NO store selector was called AND no store was selected/mentioned, this category is NOT APPLICABLE.\n",
    "- +10 Optimal Store: Award ONLY if a store was selected (via tool or mentioned) AND the store was appropriate for the user's request (e.g., has the items, good location, matches preferences).\n",
    "\n",
    "PENALTIES:\n",
    "- −50 pts if store is out of range of customer's address AND the agent USED that store anyway without informing the user\n",
    "- +0 pts (NO PENALTY) if agent correctly INFORMED the user that their requested store is unavailable and did NOT proceed with it. This is GOOD behavior.\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"points_earned\": <number (CAN BE NEGATIVE, e.g., −50 for out-of-range store)>,\n",
    "  \"max_points\": 10,\n",
    "  \"applicable\": <true if ANY store selection occurred (tool called OR store mentioned), otherwise false>,\n",
    "  \"reasoning\": \"VERIFICATION:\\\\n- Store selector called or store mentioned: [YES/NO - explain what you found in the conversation]\\\\n- Store chosen: [name or N/A]\\\\n- Location issues: [YES/NO - if yes, did agent inform user? Did agent proceed anyway?]\\\\n\\\\nSCORING:\\\\n[+10 or +0 or −50] Store appropriateness: [Provide reasoning. If store was unavailable but agent informed user and didn't proceed, award +0 (no penalty). Only penalize if agent USED unavailable store.]\\\\n\\\\nPENALTIES APPLIED: [List any penalties with evidence. NOTE: Informing user of unavailable store is NOT a penalty.]\\\\n\\\\nTOTAL: X/10 points or N/A (CAN BE NEGATIVE)\"\n",
    "}}\n",
    "\n",
    "CRITICAL: \n",
    "- Analyze the CONVERSATION to determine if store selection occurred - look for tool calls, store names, phrases like \"I found these stores\", etc.\n",
    "- Points CAN BE NEGATIVE if penalties are applied (−50 for USING an out-of-range store without informing user)\n",
    "- GOOD BEHAVIOR: If agent tells user \"Store X is not available in your area\", this is correct behavior and receives +0 pts (no penalty)\n",
    "- BAD BEHAVIOR: If agent proceeds to use an unavailable store without telling the user, this receives −50 pts\n",
    "- Only mark as N/A if NO store selection happened at all\n",
    "\"\"\"\n",
    "        return self._call_openai_with_retry(prompt)\n",
    "\n",
    "    def _grade_search_quality(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "\n",
    "        prompt = f\"\"\"Evaluate SEARCH QUALITY:\n",
    "\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "YOUR TASK: Analyze the conversation/trace data above and determine:\n",
    "1. Were any product searches performed? (Look for search queries in the items, tool calls, etc.)\n",
    "2. What search terms were used?\n",
    "3. Were the search results relevant to what the user asked for?\n",
    "\n",
    "IMPORTANT: Only grade if searches were performed. If no searches found in the trace, all criteria are N/A.\n",
    "\n",
    "SUB-CRITERIA (Max 20 pts total):\n",
    "\n",
    "1. Term Specificity (12 pts): Award 12 pts ONLY if ALL search terms/queries were reasonable and specific enough for what the user asked for. Judge ONLY the search query itself, NOT the results. If ANY search term was vague or unclear, award 0 pts. If no searches, mark as N/A.\n",
    "   CRITICAL: NO PARTIAL CREDIT - Either 12 pts (all search queries reasonable) OR 0 pts (any query too vague)\n",
    "   EXAMPLES:\n",
    "   - ✓ 12 pts: User asks for 'chicken', query='chicken' (reasonable)\n",
    "   - ✓ 12 pts: User asks for 'rice', query='rice' (reasonable)\n",
    "   - ✗ 0 pts: User asks for 'organic chicken breast', query='food' (too vague)\n",
    "   \n",
    "2. Result Relevance (8 pts): Award 8 pts ONLY if ALL searches returned correct, relevant items. Judge ONLY the search results, NOT the query. If ANY search returned wrong items, award 0 pts. If no searches, mark as N/A.\n",
    "   CRITICAL: NO PARTIAL CREDIT - Either 8 pts (all results correct) OR 0 pts (any results wrong)\n",
    "   EXAMPLES:\n",
    "   - ✗ 0 pts: Query='chicken', Results=instant noodles, dog food (wrong results)\n",
    "   - ✗ 0 pts: Query='tomatoes', Results=tomato sauce (should be fresh tomatoes)\n",
    "   - ✓ 8 pts: Query='rice', Results=plain rice bags (correct results)\n",
    "\n",
    "REQUIRED: For EACH criterion, state whether it was applied or N/A with specific evidence.\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"term_specificity\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or 12 ONLY>,\n",
    "    \"reasoning\": \"If 12 pts: 'ALL search queries were reasonable and specific. Examples: [list queries]'. If 0 pts: 'NOT all queries specific. Issues: [list vague queries]'. If N/A: 'No searches performed'. NOTE: Judge ONLY the query text, NOT whether results were good.\"\n",
    "  }},\n",
    "  \"result_relevance\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or 8 ONLY>,\n",
    "    \"reasoning\": \"If 8 pts: 'ALL search results were correct and relevant'. If 0 pts: 'NOT all results correct. Wrong items: 'chicken' query → 'instant noodles, dog food' (should be actual chicken), 'tomatoes' query → 'tomato sauce' (should be fresh tomatoes)'. If N/A: 'No searches performed'. NOTE: Judge ONLY the results, NOT the query.\"\n",
    "  }},\n",
    "  \"total_points\": <sum of all points (0, 8, 12, or 20 ONLY)>,\n",
    "  \"max_points\": 20,\n",
    "  \"penalties_applied\": [\"List issues that caused 0 pts\"],\n",
    "  \"overall_reasoning\": \"Summary: [Explain whether searches were performed]. Term Specificity: [0 or 12] pts. Result Relevance: [0 or 8] pts. TOTAL: [X]/20 pts\"\n",
    "}}\n",
    "\n",
    "CRITICAL REMINDER:\n",
    "- NO PARTIAL CREDIT - points must be 0, 8, 12, or 20 only\n",
    "- Term Specificity: Judge ONLY the search query itself (was 'chicken' reasonable? yes = 12 pts). Don't penalize if results were bad.\n",
    "- Result Relevance: Judge ONLY the search results (did 'chicken' return actual chicken? no = 0 pts). Don't penalize the query.\n",
    "- If ANY search query is too vague → 0/12 for Term Specificity\n",
    "- If ANY result is wrong → 0/8 for Result Relevance\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Post-process to calculate totals\n",
    "        term_spec = result.get('term_specificity', {})\n",
    "        result_rel = result.get('result_relevance', {})\n",
    "        \n",
    "        # Convert to int in case LLM returns strings\n",
    "        def safe_int(val):\n",
    "            if isinstance(val, (int, float)):\n",
    "                return int(val)\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        # ENFORCE BINARY SCORING: Only allow 0/12 for Term Specificity and 0/8 for Result Relevance\n",
    "        term_pts = safe_int(term_spec.get('points', 0))\n",
    "        if term_spec.get('applicable', False):\n",
    "            if term_pts > 0 and term_pts != 12:\n",
    "                term_pts = 0  # Force to 0 if not exactly 12\n",
    "                term_spec['points'] = 0\n",
    "                term_spec['reasoning'] = f\"[ENFORCED: 0 pts] NO PARTIAL CREDIT. \" + term_spec.get('reasoning', '')\n",
    "            elif term_pts < 0:\n",
    "                term_pts = 0\n",
    "                term_spec['points'] = 0\n",
    "        \n",
    "        result_pts = safe_int(result_rel.get('points', 0))\n",
    "        if result_rel.get('applicable', False):\n",
    "            if result_pts > 0 and result_pts != 8:\n",
    "                result_pts = 0  # Force to 0 if not exactly 8\n",
    "                result_rel['points'] = 0\n",
    "                result_rel['reasoning'] = f\"[ENFORCED: 0 pts] NO PARTIAL CREDIT. \" + result_rel.get('reasoning', '')\n",
    "            elif result_pts < 0:\n",
    "                result_pts = 0\n",
    "                result_rel['points'] = 0\n",
    "        \n",
    "        points_earned = term_pts + result_pts\n",
    "        \n",
    "        # Determine if searches were applicable based on whether either criterion was applicable\n",
    "        searches_applicable = term_spec.get('applicable', False) or result_rel.get('applicable', False)\n",
    "        max_points = 20 if searches_applicable else 0\n",
    "        \n",
    "        # Build detailed breakdown\n",
    "        detailed_breakdown = []\n",
    "        for name, data in [\n",
    "            ('Term Specificity', term_spec),\n",
    "            ('Result Relevance', result_rel)\n",
    "        ]:\n",
    "            if data and data.get('applicable', False):\n",
    "                pts = data.get('points', 0)\n",
    "                reasoning = data.get('reasoning', '')\n",
    "                detailed_breakdown.append(f\"{'✓' if pts > 0 else '✗'} {name}: {pts:+d} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = data.get('reasoning', 'Not applicable') if data else 'Not applicable'\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        penalties = result.get('penalties_applied', [])\n",
    "        penalties_text = \"\\\\n\\\\nPENALTIES APPLIED:\\\\n\" + \"\\\\n\".join(f\"  • {p}\" for p in penalties) if penalties else \"\"\n",
    "        \n",
    "        return {\n",
    "            'points_earned': points_earned,\n",
    "            'max_points': max_points,\n",
    "            'applicable': searches_applicable,\n",
    "            'reasoning': \"\\\\n\".join(detailed_breakdown) + penalties_text\n",
    "        }\n",
    "\n",
    "    def _grade_pick_accuracy(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        \n",
    "        # Extract all items that were added to shopping list across all turns\n",
    "        all_items = []\n",
    "        for turn in trace.get('turns', []):\n",
    "            items = turn.get('items', [])\n",
    "            for item in items:\n",
    "                query = item.get('query', 'Unknown query')\n",
    "                # The selected item is stored directly in item_name, not in options\n",
    "                selected_name = item.get('item_name', 'N/A')\n",
    "                all_items.append(f\"- Query: '{query}' → Selected: {selected_name}\")\n",
    "        \n",
    "        has_items = bool(all_items)\n",
    "        items_summary = \"\\n\".join(all_items[:15]) if has_items else \"No items were selected/picked.\"\n",
    "\n",
    "        prompt = f\"\"\"Evaluate PICK ACCURACY:\n",
    "\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "ITEMS SELECTED/PICKED DURING SHOPPING LIST CREATION:\n",
    "{items_summary}\n",
    "\n",
    "IMPORTANT: Only grade criteria that are applicable. If no items were selected, all criteria are N/A.\n",
    "\n",
    "SUB-CRITERIA (Max 8 pts total):\n",
    "1. Product Type Match (4 pts): Award +4 pts ONLY if ALL items match correct product category. If ANY item is wrong product type, award 0 pts and apply SINGLE −5 penalty. If no items selected, mark as N/A.\n",
    "   PENALTY: −5 pts applied ONCE if ANY wrong product type exists (e.g., \"tomatoes\" → \"tomato sauce\")\n",
    "   Example: 10 items, 7 correct, 3 wrong → Score: 0 + (−5) = −5 pts (NOT −15, penalty applied once)\n",
    "   \n",
    "2. Attributes Match (3 pts): Award +3 pts ONLY if ALL items have correct size/flavor/brand (within reasonable tolerance). If ANY item has wrong attributes, award 0 pts and apply SINGLE −5 penalty. If no specific attributes requested, mark as N/A.\n",
    "   PENALTY: −5 pts applied ONCE if ANY size/variant is wrong beyond tolerance\n",
    "   \n",
    "3. Explanation Quality (1 pt): Award +1 pt if agent explained their picks. 0 pts if no explanations. If no items selected, mark as N/A.\n",
    "\n",
    "CRITICAL - BINARY SCORING:\n",
    "- Product Type: Either +4 (all correct) OR 0 + (−5) = −5 (any wrong)\n",
    "- Attributes: Either +3 (all correct) OR 0 + (−5) = −5 (any wrong)\n",
    "- Explanation: Either +1 (explained) OR 0 (didn't explain)\n",
    "- Penalties are BINARY (applied ONCE), NOT per-item\n",
    "- Possible scores: 8, 7, 4, 3, 2, 1, 0, -1, -2, -4, -5, -9, -10\n",
    "\n",
    "REQUIRED: For EACH criterion, state whether it was applied or N/A and WHY.\n",
    "Example (all correct): \"[+4] Product Type Match: ALL items correct\"\n",
    "Example (some wrong): \"[−5] Product Type Match: 0/4 awarded + SINGLE −5 penalty. Wrong items found: 'tomatoes' → 'tomato sauce', 'rice' → 'prepared meal'. Total: −5 pts\"\n",
    "Example: \"[N/A] Product Type Match: NOT APPLICABLE - No items were selected\"\n",
    "Example (attributes all match): \"[+3] Attributes Match: ALL sizes/brands match within tolerance\"\n",
    "Example (attributes wrong): \"[−5] Attributes Match: 0/3 awarded + SINGLE −5 penalty. Wrong: 'almond milk' size mismatch. Total: −5 pts\"\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"product_type_match\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <4, 0, or -5 ONLY>, \n",
    "    \"reasoning\": \"If all correct: 'ALL items match correct product type (+4 pts)'. If any wrong: '0/4 pts + SINGLE −5 penalty. Wrong items: [list examples]. Total: −5 pts'\"\n",
    "  }},\n",
    "  \"attributes_match\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <3, 0, or -5 ONLY>, \n",
    "    \"reasoning\": \"If all correct: 'ALL attributes match (+3 pts)'. If any wrong: '0/3 pts + SINGLE −5 penalty. Wrong: [list examples]. Total: −5 pts'. If N/A: 'No specific attributes requested'\"\n",
    "  }},\n",
    "  \"explanation_quality\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <0 or 1 ONLY>, \n",
    "    \"reasoning\": \"Either 'Agent explained picks (+1 pt)' OR 'No explanations provided (0 pts)'\"\n",
    "  }},\n",
    "  \"total_points\": <sum: 8, 7, 4, 3, 2, 1, 0, -1, -2, -4, -5, -9, or -10 ONLY>,\n",
    "  \"max_points\": 8,\n",
    "  \"penalties_applied\": [\"Product Type: Wrong items found (e.g., 'tomatoes' → 'tomato sauce')\", \"Attributes: Size/brand mismatch found\"],\n",
    "  \"overall_reasoning\": \"Summary: X items selected. Product Type: [+4, 0, or −5]. Attributes: [+3, 0, or −5]. Explanation: [+1 or 0]. TOTAL: [X]/8 pts\"\n",
    "}}\n",
    "\n",
    "CRITICAL:\n",
    "- Penalties are BINARY (applied ONCE), not per-item\n",
    "- Product Type: +4 (all correct), 0 (some wrong but no penalty), or −5 (some wrong + penalty)\n",
    "- Attributes: +3 (all correct), 0 (some wrong but no penalty OR N/A), or −5 (some wrong + penalty)\n",
    "- DO NOT give multiple −5 penalties, only ONE per category\n",
    "\n",
    "CRITICAL: \n",
    "- Evaluate items that were SELECTED during list building, NOT final cart state\n",
    "- Mark criteria as N/A if not applicable (e.g., no items selected, no attributes specified, no explanations given)\n",
    "- List ALL penalties in the penalties_applied array with specific examples\n",
    "- Total points = sum of all sub-criteria (can be negative if penalties exceed points)\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Convert to int in case LLM returns strings\n",
    "        def safe_int(val):\n",
    "            if isinstance(val, (int, float)):\n",
    "                return int(val)\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        # ENFORCE BINARY PENALTIES: Product Type can only be 4, 0, or -5; Attributes can only be 3, 0, or -5\n",
    "        product_type = result.get('product_type_match', {})\n",
    "        if product_type.get('applicable', False):\n",
    "            pts = safe_int(product_type.get('points', 0))\n",
    "            # Enforce binary scoring: must be exactly 4, 0, or -5\n",
    "            if pts not in [4, 0, -5]:\n",
    "                if pts > 4:\n",
    "                    pts = 4\n",
    "                elif pts < -5:\n",
    "                    pts = -5\n",
    "                elif pts > 0:  # Between 1-3\n",
    "                    pts = 0\n",
    "                else:  # Between -4 and -1\n",
    "                    pts = -5\n",
    "                product_type['points'] = pts\n",
    "                product_type['reasoning'] = f\"[ENFORCED: {pts} pts] \" + product_type.get('reasoning', '')\n",
    "                result['product_type_match'] = product_type\n",
    "        \n",
    "        attributes = result.get('attributes_match', {})\n",
    "        if attributes.get('applicable', False):\n",
    "            pts = safe_int(attributes.get('points', 0))\n",
    "            # Enforce binary scoring: must be exactly 3, 0, or -5\n",
    "            if pts not in [3, 0, -5]:\n",
    "                if pts > 3:\n",
    "                    pts = 3\n",
    "                elif pts < -5:\n",
    "                    pts = -5\n",
    "                elif pts > 0:  # Between 1-2\n",
    "                    pts = 0\n",
    "                else:  # Between -4 and -1\n",
    "                    pts = -5\n",
    "                attributes['points'] = pts\n",
    "                attributes['reasoning'] = f\"[ENFORCED: {pts} pts] \" + attributes.get('reasoning', '')\n",
    "                result['attributes_match'] = attributes\n",
    "        \n",
    "        # Calculate points based on applicable sub-criteria and build detailed reasoning\n",
    "        points_earned = 0\n",
    "        max_points = 0\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'product_type_match': ('Product Type Match', 4),\n",
    "            'attributes_match': ('Attributes Match', 3),\n",
    "            'explanation_quality': ('Explanation Quality', 1)\n",
    "        }\n",
    "        \n",
    "        for criterion, (name, max_pts) in criterion_names.items():\n",
    "            if criterion in result and result[criterion].get('applicable', False):\n",
    "                pts = safe_int(result[criterion].get('points', 0))\n",
    "                reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                points_earned += pts\n",
    "                max_points += max_pts\n",
    "                detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        penalties = result.get('penalties_applied', [])\n",
    "        penalties_text = \"\\\\n\\\\nPENALTIES APPLIED:\\\\n\" + \"\\\\n\".join(f\"  • {p}\" for p in penalties) if penalties else \"\"\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['applicable'] = max_points > 0  # Only applicable if at least one criterion applies\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + penalties_text + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _grade_shopping_list_building(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        \n",
    "        # Check if multi-turn\n",
    "        is_multi_turn = trace.get('total_turns', 1) > 1\n",
    "        \n",
    "        # Count items actually added to the list\n",
    "        total_items_added = 0\n",
    "        for turn in trace.get('turns', []):\n",
    "            items = turn.get('items', [])\n",
    "            total_items_added += len(items)\n",
    "        \n",
    "        # Check if list creation was attempted but failed\n",
    "        shopping_list_created = trace.get('shopping_list_created', False)\n",
    "        list_creation_attempted = any(\n",
    "            'createShoppingList' in str(turn.get('tool_calls', []))\n",
    "            for turn in trace.get('turns', [])\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"Evaluate SHOPPING LIST BUILDING:\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "MULTI-TURN: {is_multi_turn}\n",
    "ITEMS ACTUALLY ADDED TO LIST: {total_items_added}\n",
    "LIST CREATION ATTEMPTED: {list_creation_attempted}\n",
    "LIST CREATION SUCCEEDED: {shopping_list_created}\n",
    "\n",
    "CRITICAL VALIDATION:\n",
    "- If list creation was attempted but 0 items were added, this is a FAILURE. Award 0 points and explain the failure.\n",
    "- If items were successfully added, proceed with normal grading.\n",
    "\n",
    "IMPORTANT: Only grade criteria that are applicable. Single-turn conversations shouldn't lose points for not demonstrating multi-turn skills.\n",
    "\n",
    "SUB-CRITERIA:\n",
    "1. Remembers Edits (5 pts): Award if agent remembers changes across turns. If single-turn (no edits possible), mark as N/A.\n",
    "   PENALTY: −5 pts if agent forgets changes/edits from previous turns\n",
    "   \n",
    "2. Follows Add/Remove (5 pts): Award if agent accurately adds/removes items. If no add/remove requests, mark as N/A.\n",
    "   PENALTY: −10 pts if agent adds random items not requested by user\n",
    "   \n",
    "3. No Duplicates/Conflicts (5 pts): Award if final list is clean AND has items. If 0 items were added despite user request, award 0 points.\n",
    "   PENALTY: −15 pts for unreasonable/extra items that don't match the goal\n",
    "   \n",
    "4. Error Handling (2 pts): Award if agent handles out-of-stock/errors gracefully. If no errors occurred, mark as N/A.\n",
    "   Note: If list creation failed (0 items added), this becomes applicable - check if agent acknowledged the error.\n",
    "\n",
    "ADDITIONAL PENALTIES:\n",
    "- −5 pts if agent breaks budget constraint (when budget was specified by user)\n",
    "- AUTOMATIC 0 TOTAL if list creation was attempted but completely failed (0 items added despite user requesting items)\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"remembers_edits\": {{\n",
    "    \"applicable\": <true/false based on multi-turn>, \n",
    "    \"points\": <number (CAN BE NEGATIVE, e.g., −5 for forgetting edits)>, \n",
    "    \"reasoning\": \"If multi-turn: detailed explanation with examples of edits. Apply −5 penalty if forgets. If single-turn: 'N/A: Single-turn conversation, no opportunity to demonstrate edit memory'\"\n",
    "  }},\n",
    "  \"follows_add_remove\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <number (CAN BE NEGATIVE, e.g., −10 for random items)>, \n",
    "    \"reasoning\": \"If add/remove requests present: detailed explanation with quotes. Apply −10 penalty for random items. Otherwise: 'N/A: No explicit add/remove requests made'\"\n",
    "  }},\n",
    "  \"no_duplicates\": {{\n",
    "    \"applicable\": true, \n",
    "    \"points\": <number (0 if list is empty, CAN BE NEGATIVE e.g., −15 for unreasonable items)>, \n",
    "    \"reasoning\": \"Detailed analysis: If {total_items_added} items were added, check for duplicates/conflicts. If 0 items added despite user request, award 0 points with explanation. Apply −15 penalty for unreasonable/extra items. List any issues found or confirm clean list.\"\n",
    "  }},\n",
    "  \"error_handling\": {{\n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <0-2>, \n",
    "    \"reasoning\": \"If errors occurred: detailed explanation of handling. Otherwise: 'N/A: No errors encountered'\"\n",
    "  }},\n",
    "  \"budget_penalty\": {{\n",
    "    \"applied\": <true/false>,\n",
    "    \"points\": <0 or −5>,\n",
    "    \"reasoning\": \"−5 if budget constraint was broken (when specified)\"\n",
    "  }},\n",
    "  \"points_earned\": <sum of points from applicable criteria (CAN BE NEGATIVE)>,\n",
    "  \"max_points\": <sum of max points from applicable criteria>,\n",
    "  \"applicable\": true,\n",
    "  \"reasoning\": \"CONTEXT: {is_multi_turn}-turn conversation\\\\n\\\\nDETAILED SCORING:\\\\n[Remembers Edits: X/5 or N/A or −5] - [detailed reasoning]\\\\n[Add/Remove: X/5 or N/A or −10] - [detailed reasoning]\\\\n[No Duplicates: X/5 or −15] - [detailed reasoning]\\\\n[Error Handling: X/2 or N/A] - [detailed reasoning]\\\\n[Budget: 0 or −5] - [detailed reasoning if applicable]\\\\n\\\\nPENALTIES APPLIED: [List all penalties]\\\\n\\\\nTOTAL: X/Y points (CAN BE NEGATIVE)\"\n",
    "}}\n",
    "\n",
    "CRITICAL: \n",
    "- Points CAN BE NEGATIVE if penalties are applied (−5 forgets edits, −10 random items, −15 unreasonable items, −5 budget violation)\n",
    "- In \"PENALTIES APPLIED\" section, MUST explain WHY each penalty was or was NOT applied with specific evidence\n",
    "- Format: \"• −5 Forgets Edits: [APPLIED/NOT APPLIED] - [Quote from conversation showing why]\"\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Calculate points based on applicable sub-criteria and build detailed reasoning\n",
    "        points_earned = 0\n",
    "        max_points = 0\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'remembers_edits': ('Remembers Edits', 5),\n",
    "            'follows_add_remove': ('Follows Add/Remove', 5),\n",
    "            'no_duplicates': ('No Duplicates/Conflicts', 5),\n",
    "            'error_handling': ('Error Handling', 2)\n",
    "        }\n",
    "        \n",
    "        for criterion, (name, max_pts) in criterion_names.items():\n",
    "            if criterion in result and result[criterion].get('applicable', False):\n",
    "                pts = result[criterion].get('points', 0)\n",
    "                reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                points_earned += pts\n",
    "                max_points += max_pts\n",
    "                detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _grade_apply_to_cart(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        \n",
    "        # Fixed: Handle multiple tool_call formats\n",
    "        def check_tool_call(tool_call):\n",
    "            if isinstance(tool_call, dict):\n",
    "                # Check 'function' field (could be dict or string)\n",
    "                func = tool_call.get('function')\n",
    "                if isinstance(func, dict):\n",
    "                    # Nested: {\"function\": {\"name\": \"applyShoppingList\"}}\n",
    "                    if func.get('name') == 'applyShoppingList':\n",
    "                        return True\n",
    "                elif isinstance(func, str):\n",
    "                    # Flat: {\"function\": \"applyShoppingList\"}\n",
    "                    if func == 'applyShoppingList':\n",
    "                        return True\n",
    "                \n",
    "                # Also check 'tool_name' field as backup\n",
    "                tool_name = tool_call.get('tool_name')\n",
    "                if tool_name == 'applyShoppingList':\n",
    "                    return True\n",
    "                    \n",
    "            elif isinstance(tool_call, str):\n",
    "                # If it's a string, check if it contains the function name\n",
    "                if 'applyShoppingList' in tool_call:\n",
    "                    return True\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        cart_applied = any(\n",
    "            check_tool_call(tool_call)\n",
    "            for turn in trace.get('turns', [])\n",
    "            for tool_call in turn.get('tool_calls', [])\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"Evaluate APPLY TO CART:\n",
    "\n",
    "CONVERSATION (includes all tool_calls and tool_results - analyze them carefully): {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "IMPORTANT: \n",
    "- The conversation above includes tool_results which show whether cart additions succeeded or failed\n",
    "- Look for \"success\": false or \"error\": \"Failed to add items to cart\" in tool_results to detect cart errors\n",
    "- Only grade if applyShoppingList tool was called. If not called, all criteria are N/A.\n",
    "\n",
    "SUB-CRITERIA (Max 7 pts total):\n",
    "\n",
    "1. Waits for Confirmation (4 pts): Award if agent waited for explicit user confirmation before adding to cart. If tool not called, mark as N/A.\n",
    "   \n",
    "   PENALTY: −20 pts if agent adds to cart WITHOUT any user request or adds too early (before user confirms)\n",
    "   \n",
    "   ✅ These count as confirmation: \"Add these\", \"Add to cart\", \"Put in my cart\", \"Yes [to adding]\", any explicit add instruction\n",
    "   ❌ Penalize −20 if: Agent adds with NO user request, or user only said \"show me\" without \"add\"\n",
    "\n",
    "2. Cart Matches List (1 pt): Award if items in cart match the confirmed shopping list. If tool not called, mark as N/A.\n",
    "   NOTE: Even if cart failed due to error, evaluate whether agent TRIED to pass correct items to the tool\n",
    "   \n",
    "3. Quantities Correct (1 pt): Award if quantities in cart match exactly what was on the list. If tool not called or no quantities to check, mark as N/A.\n",
    "   ⚠️ CRITICAL: Analyze tool_results in the conversation to detect cart errors\n",
    "   - If you see {{\"success\": false, \"error\": \"Failed to add items to cart\"}} in tool_results → give −20 pts\n",
    "   - If final_cart is empty OR has 0 items → give −20 pts  \n",
    "   - Reasoning: If cart checkout failed, nothing was actually added to cart, so quantities are objectively wrong!\n",
    "\n",
    "4. Summarizes (1 pt): Award if agent summarized what was added to cart. If tool not called, mark as N/A.\n",
    "\n",
    "REQUIRED: For EACH criterion, state whether it was applied or N/A with specific evidence from the conversation.\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"waits_for_confirmation\": {{ \"applicable\": <true/false>, \"points\": <−20 to 4>, \"reasoning\": \"...\" }},\n",
    "  \"cart_matches_list\": {{ \"applicable\": <true/false>, \"points\": <−20 to 1>, \"reasoning\": \"Evaluate whether items agent tried to pass to cart match the list (even if cart failed)\" }},\n",
    "  \"quantities_correct\": {{ \n",
    "    \"applicable\": <true/false>, \n",
    "    \"points\": <−20 to 1>, \n",
    "    \"reasoning\": \"Check tool_results for cart errors. If 'success': false or 'error': 'Failed to add items to cart', give −20 pts since nothing was actually added to cart.\" \n",
    "  }},\n",
    "  \"summarizes\": {{ \"applicable\": <true/false>, \"points\": <0-1>, \"reasoning\": \"...\" }},\n",
    "  \"total_points\": <sum of all points (CAN BE NEGATIVE)>,\n",
    "  \"max_points\": 7,\n",
    "  \"penalties_applied\": [\"List each penalty with reasoning\"],\n",
    "  \"overall_reasoning\": \"Summary: Analyze whether applyShoppingList was called and whether it succeeded or failed. [X/4] Waits + [X/1] Cart Matches + [X/1] Quantities + [X/1] Summarizes = X/7 pts\"\n",
    "}}\n",
    "\n",
    "IMPORTANT GRADING GUIDANCE:\n",
    "- Carefully analyze tool_results in the conversation to detect cart failures\n",
    "- Look for {{\"success\": false, \"error\": \"Failed to add items to cart\"}} in tool_results\n",
    "- Look for empty final_cart or cart with 0 items\n",
    "- If cart checkout failed → give −20 pts for Quantities Correct (nothing was added, so quantities are wrong)\n",
    "- Cart Matches can be evaluated normally (what agent tried to pass)\n",
    "- Example: tool_results shows \"success\": false and \"Failed to add items to cart\" → Quantities should be −20 pts\n",
    "- However, use your judgment if there are extenuating circumstances\n",
    "\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Post-process to calculate totals\n",
    "        waits = result.get('waits_for_confirmation', {})\n",
    "        cart_match = result.get('cart_matches_list', {})\n",
    "        quantities = result.get('quantities_correct', {})\n",
    "        summarizes = result.get('summarizes', {})\n",
    "        \n",
    "        # Note: No hard-coded enforcement - let LLM decide based on strong prompting above\n",
    "        \n",
    "        # Convert to int in case LLM returns strings\n",
    "        def safe_int(val):\n",
    "            if isinstance(val, (int, float)):\n",
    "                return int(val)\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        points_earned = sum([\n",
    "            safe_int(waits.get('points', 0)),\n",
    "            safe_int(cart_match.get('points', 0)),\n",
    "            safe_int(quantities.get('points', 0)),\n",
    "            safe_int(summarizes.get('points', 0))\n",
    "        ])\n",
    "        \n",
    "        max_points = 7 if cart_applied else 0\n",
    "        \n",
    "        # Build detailed breakdown\n",
    "        detailed_breakdown = []\n",
    "        for name, data in [\n",
    "            ('Waits for Confirmation', waits),\n",
    "            ('Cart Matches List', cart_match),\n",
    "            ('Quantities Correct', quantities),\n",
    "            ('Summarizes', summarizes)\n",
    "        ]:\n",
    "            if data and data.get('applicable', False):\n",
    "                pts = data.get('points', 0)\n",
    "                reasoning = data.get('reasoning', '')\n",
    "                detailed_breakdown.append(f\"{'✓' if pts > 0 else '✗'} {name}: {pts:+d} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = data.get('reasoning', 'Not applicable') if data else 'Not applicable'\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        penalties = result.get('penalties_applied', [])\n",
    "        penalties_text = \"\\\\n\\\\nPENALTIES APPLIED:\\\\n\" + \"\\\\n\".join(f\"  • {p}\" for p in penalties) if penalties else \"\"\n",
    "        \n",
    "        return {\n",
    "            'points_earned': points_earned,\n",
    "            'max_points': max_points,\n",
    "            'applicable': cart_applied,\n",
    "            'reasoning': \"\\\\n\".join(detailed_breakdown) + penalties_text\n",
    "        }\n",
    "\n",
    "    def _grade_personalization_tone(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        \n",
    "        has_saved_prefs = bool(trace.get('user_profile'))\n",
    "\n",
    "        prompt = f\"\"\"Evaluate PERSONALIZATION & TONE:\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "USER HAS SAVED PREFERENCES: {has_saved_prefs}\n",
    "\n",
    "IMPORTANT: Only grade criteria that are applicable. Mark as N/A if not applicable.\n",
    "\n",
    "SUB-CRITERIA:\n",
    "1. Uses Saved Preferences (5 pts): Award if agent uses pre-existing user preferences (diet, brands, etc.). If no saved preferences exist, mark as N/A.\n",
    "   PENALTY: −5 pts if agent ignores saved preferences (e.g., suggests non-vegan items to vegan user)\n",
    "   \n",
    "2. Learns New Preferences (4 pts): Award if agent learns and applies a NEW preference stated DURING this conversation. If no new preferences were stated, mark as N/A.\n",
    "   \n",
    "3. Polite & Helpful Tone (2 pts): Award if tone is consistently polite, helpful, and not argumentative. Always applicable.\n",
    "   PENALTY: −2 pts if tone is off-brand, overly wordy, or bickering with consumer\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"uses_saved_preferences\": {{\n",
    "    \"applicable\": <true if saved prefs exist, false otherwise>, \n",
    "    \"points\": <number (CAN BE NEGATIVE, e.g., −5 for ignoring prefs)>, \n",
    "    \"reasoning\": \"If saved prefs exist: explain how they were used with examples. Apply −5 penalty if ignores preferences. If not: 'N/A: No saved preferences available'\"\n",
    "  }},\n",
    "  \"learns_new_preferences\": {{\n",
    "    \"applicable\": <true if new prefs were stated in conversation, false otherwise>, \n",
    "    \"points\": <0-4>, \n",
    "    \"reasoning\": \"If new preferences stated: explain how they were learned/applied. If not: 'N/A: No new preferences stated during conversation'\"\n",
    "  }},\n",
    "  \"polite_helpful_tone\": {{\n",
    "    \"applicable\": true, \n",
    "    \"points\": <number (CAN BE NEGATIVE, e.g., −2 for off-brand/wordy tone)>, \n",
    "    \"reasoning\": \"Analyze tone with specific quotes from the conversation. Apply −2 penalty if off-brand, overly wordy, or bickering\"\n",
    "  }},\n",
    "  \"points_earned\": <sum of points from applicable criteria (CAN BE NEGATIVE)>,\n",
    "  \"max_points\": <sum of max points from applicable criteria>,\n",
    "  \"applicable\": true,\n",
    "  \"reasoning\": \"DETAILED SCORING:\\\\n[Saved Prefs: X/5 or N/A or −5] - [reasoning]\\\\n[Learned Prefs: X/4 or N/A] - [reasoning]\\\\n[Tone: X/2 or −2] - [reasoning]\\\\n\\\\nPENALTIES APPLIED: [List all penalties]\\\\n\\\\nTOTAL: X/Y points (CAN BE NEGATIVE)\"\n",
    "}}\n",
    "\n",
    "CRITICAL: \n",
    "- Points CAN BE NEGATIVE if penalties are applied (−5 for ignoring preferences, −2 for tone issues)\n",
    "- In \"PENALTIES APPLIED\" section, explain each penalty decision with quotes from conversation\n",
    "- Format: \"• −5 Ignores Preferences: [APPLIED/NOT APPLIED] - [Evidence]\"\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Calculate points based on applicable sub-criteria and build detailed reasoning\n",
    "        points_earned = 0\n",
    "        max_points = 0\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'uses_saved_preferences': ('Uses Saved Preferences', 5),\n",
    "            'learns_new_preferences': ('Learns New Preferences', 4),\n",
    "            'polite_helpful_tone': ('Polite & Helpful Tone', 2)\n",
    "        }\n",
    "        \n",
    "        for criterion, (name, max_pts) in criterion_names.items():\n",
    "            if criterion in result and result[criterion].get('applicable', False):\n",
    "                pts = result[criterion].get('points', 0)\n",
    "                reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                points_earned += pts\n",
    "                max_points += max_pts\n",
    "                detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _grade_reliability(self, trace: Dict) -> Dict:\n",
    "        \"\"\"Simplified reliability check for a single trace.\"\"\"\n",
    "        # For a single conversation, we assume reliability if the main goal is met.\n",
    "        # This is a proxy for more complex reliability testing across different phrasings.\n",
    "        # The logic defaults to awarding full points if the conversation is successful.\n",
    "        return {\n",
    "            \"points_earned\": 3,\n",
    "            \"max_points\": 3,\n",
    "            \"applicable\": True,\n",
    "            \"reasoning\": \"Single trace evaluation assumes reliability if the task was completed. Awarded default points.\",\n",
    "            \"criteria_met\": [\"Successfully handled user requests\"]\n",
    "        }\n",
    "\n",
    "    def _grade_goal_completion(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "        final_cart_items = len(trace.get('turns', [{}])[-1].get('final_cart', []))\n",
    "        turn_count = trace.get('total_turns', len(trace.get('turns', [])))\n",
    "\n",
    "        prompt = f\"\"\"Evaluate GOAL COMPLETION:\n",
    "\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "TURN COUNT: {turn_count}\n",
    "\n",
    "IMPORTANT: Only grade criteria that are applicable. This category is ALWAYS applicable.\n",
    "\n",
    "SUB-CRITERIA (Max 5 pts total):\n",
    "1. Goal Fulfillment (5 pts): BINARY - Award 5 pts if agent FULLY achieved user's goal, 0 pts otherwise. Always applicable.\n",
    "   - Shopping list created AND matches user's goal\n",
    "   - All required items included\n",
    "   - User confirmed success OR conversation ended successfully\n",
    "   - If items were supposed to be added to cart, they MUST be in cart\n",
    "   NO PARTIAL CREDIT: Either goal is fully met (5 pts) or not met (0 pts)\n",
    "   \n",
    "2. Timeout Penalty (−5 pts): Apply ONLY if conversation took excessive turns (>10 turns). If reasonable turn count, mark as N/A.\n",
    "   PENALTY: −5 pts if conversation exceeds 10 turns before completion\n",
    "\n",
    "REQUIRED: For EACH criterion, you MUST explicitly state whether it was applied or N/A and WHY with specific evidence from the conversation.\n",
    "Example: \"[+5] Goal Fulfillment: AWARDED - User wanted recipe ingredients for pasta, agent created complete shopping list with all items (spaghetti, cream, bacon, cheese), user confirmed 'perfect!'\"\n",
    "Example: \"[+0] Goal Fulfillment: NOT AWARDED - User wanted 5 items but only 3 were added to list\"\n",
    "Example: \"[−5] Timeout Penalty: APPLIED - Conversation took 12 turns with repeated back-and-forth\"\n",
    "Example: \"[N/A] Timeout Penalty: NOT APPLICABLE - Conversation completed in 2 turns, well under threshold\"\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"goal_fulfillment\": {{\n",
    "    \"applicable\": true,\n",
    "    \"points\": <0 or 5 ONLY>,\n",
    "    \"reasoning\": \"Detailed explanation of whether goal was FULLY met with specific evidence from conversation\"\n",
    "  }},\n",
    "  \"timeout_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −5 ONLY>,\n",
    "    \"reasoning\": \"Explanation of turn count ({turn_count} turns) and whether it exceeded threshold, or 'N/A: Reasonable turn count'\"\n",
    "  }},\n",
    "  \"total_points\": <sum of all points (5, 0, or −5 ONLY)>,\n",
    "  \"max_points\": 5,\n",
    "  \"penalties_applied\": [\"List penalty if timeout occurred\", \"e.g., −5 pts: Excessive turns (12 turns > 10 turn threshold)\"],\n",
    "  \"overall_reasoning\": \"Summary: User's goal was [state goal]. Goal was either fully met or not met. Conversation took {turn_count} turns. [X/5] Goal Fulfillment + [X] Timeout Penalty = X/5 pts total\"\n",
    "}}\n",
    "\n",
    "CRITICAL:\n",
    "- NO PARTIAL CREDIT for goal fulfillment - MUST be exactly 5 or 0\n",
    "- Timeout penalty ONLY applies if >10 turns, otherwise mark as N/A\n",
    "- Total can only be: 5 (goal met, no timeout), 0 (goal not met), or −5 (goal not met + timeout)\n",
    "- Provide specific evidence from conversation for both criteria\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Convert to int in case LLM returns strings\n",
    "        def safe_int(val):\n",
    "            if isinstance(val, (int, float)):\n",
    "                return int(val)\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        # Calculate points based on sub-criteria\n",
    "        points_earned = 0\n",
    "        max_points = 0\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'goal_fulfillment': ('Goal Fulfillment', 5),\n",
    "            'timeout_penalty': ('Timeout Penalty', 0)  # Penalty, so max is 0\n",
    "        }\n",
    "        \n",
    "        for criterion, (name, max_pts) in criterion_names.items():\n",
    "            if criterion in result and result[criterion].get('applicable', False):\n",
    "                pts = safe_int(result[criterion].get('points', 0))\n",
    "                reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                if criterion == 'goal_fulfillment':\n",
    "                    points_earned += pts\n",
    "                    max_points += max_pts\n",
    "                    detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "                else:\n",
    "                    # Timeout is a penalty\n",
    "                    points_earned += pts  # Will be negative or 0\n",
    "                    detailed_breakdown.append(f\"✓ {name}: {pts} pts\\\\n  {reasoning}\")\n",
    "            else:\n",
    "                reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        # For goal completion, max_points is always 5 since it's always applicable\n",
    "        max_points = 5\n",
    "        \n",
    "        penalties = result.get('penalties_applied', [])\n",
    "        penalties_text = \"\\\\n\\\\nPENALTIES APPLIED:\\\\n\" + \"\\\\n\".join(f\"  • {p}\" for p in penalties) if penalties else \"\"\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['applicable'] = True  # Always applicable\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + penalties_text + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def _grade_clarifying_questions(self, trace: Dict) -> Dict:\n",
    "        conversation = self._serialize_conversation(trace)\n",
    "        user_prefs = self._format_user_preferences(trace)\n",
    "\n",
    "        prompt = f\"\"\"Evaluate CLARIFYING QUESTIONS:\n",
    "\n",
    "CONVERSATION: {conversation}\n",
    "{user_prefs}\n",
    "\n",
    "IMPORTANT: If NO ambiguity existed in user's requests, all positive criteria are N/A and full 6/6 points awarded automatically.\n",
    "\n",
    "SUB-CRITERIA (Max 6 pts total):\n",
    "\n",
    "POSITIVE CRITERIA (only applicable if ambiguity exists):\n",
    "1. Asked When Needed (3 pts): Award ONLY if there WAS ambiguity AND agent asked appropriate clarifying questions. If no ambiguity, mark as N/A.\n",
    "   \n",
    "2. References Context (2 pts): Award ONLY if agent's clarifying question referenced known context (preferences, previous choices) and proposed defaults. If no ambiguity or no context reference, mark as N/A.\n",
    "   \n",
    "3. Resolved Quickly (1 pt): Award ONLY if ambiguity was resolved within reasonable turns and decision captured in list/cart. If no ambiguity or not resolved, mark as N/A.\n",
    "\n",
    "PENALTY CRITERIA (can apply even without ambiguity):\n",
    "4. Didn't Ask Penalty (−5 pts): Apply ONLY if there WAS clear ambiguity BUT agent didn't ask clarifying question. If no ambiguity, mark as N/A.\n",
    "\n",
    "5. Asked After Acting Penalty (−5 pts): Apply ONLY if agent took action (added to cart/list) BEFORE asking for needed clarification. If no such issue, mark as N/A.\n",
    "\n",
    "6. Leading/Vague Questions Penalty (−4 pts): Apply ONLY if agent asked questions that were leading, vague, or introduced NEW ambiguity. If questions were clear, mark as N/A.\n",
    "\n",
    "7. Repeated Questions Penalty (−5 pts): Apply ONLY if agent asked same questions multiple times without progress or took excessive turns. If no repetition, mark as N/A.\n",
    "\n",
    "8. Didn't Repeat Back Penalty (−3 pts): Apply ONLY if agent failed to repeat back or confirm user's request to ensure understanding. If agent confirmed understanding, mark as N/A.\n",
    "\n",
    "CRITICAL REQUIREMENT FOR DETAILED REASONING:\n",
    "For EACH of the 8 criteria above, you MUST provide EXTENSIVE explanation with:\n",
    "- Specific quotes from the conversation\n",
    "- Turn numbers where events occurred\n",
    "- Exact explanation of why points were awarded or not awarded\n",
    "- If N/A, explain WHY it doesn't apply with evidence\n",
    "\n",
    "Example DETAILED reasoning:\n",
    "\"[+3] Asked When Needed: AWARDED - In Turn 2, user said 'I need milk' which is ambiguous (type? size? brand?). Agent immediately asked 'Would you like whole milk, 2%, or almond milk? I see you've purchased almond milk before.' This is appropriate clarification at the right time.\"\n",
    "\n",
    "Example DETAILED N/A:\n",
    "\"[N/A] Asked When Needed: NOT APPLICABLE - User was completely specific: 'Add Organic Valley Whole Milk 1 gallon to my cart.' No ambiguity exists - brand, type, and size all specified. Therefore asking clarifying questions would be unnecessary.\"\n",
    "\n",
    "Example DETAILED penalty:\n",
    "\"[−5] Didn't Ask Penalty: PENALTY APPLIED - In Turn 1, user said 'add chicken' which is highly ambiguous (whole chicken? breasts? thighs? how much?). Agent immediately added 'Chicken Breast 1lb' in Turn 2 without asking ANY clarifying questions. This is a clear case where clarification was needed but not requested.\"\n",
    "\n",
    "Return JSON with this EXACT structure:\n",
    "{{\n",
    "  \"has_ambiguity\": <true/false>,\n",
    "  \"asked_when_needed\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0-3>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (3-5 sentences minimum) with specific quotes from conversation, turn numbers, and detailed analysis of whether agent asked clarifying questions when ambiguity existed. Include examples of ambiguous requests and agent's response.\"\n",
    "  }},\n",
    "  \"references_context\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0-2>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (3-5 sentences minimum) with specific quotes showing whether agent referenced user preferences, past purchases, or context when asking questions. Include what context was available and how it was used.\"\n",
    "  }},\n",
    "  \"resolved_quickly\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0-1>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (2-4 sentences minimum) describing how ambiguity was resolved, how many turns it took, and whether the decision was captured in the list/cart. Include turn numbers and outcome.\"\n",
    "  }},\n",
    "  \"didnt_ask_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −5>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (3-5 sentences minimum) analyzing whether there were ambiguous requests where agent should have asked but didn't. Provide specific examples with quotes and explain the ambiguity that existed.\"\n",
    "  }},\n",
    "  \"asked_after_acting_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −5>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (3-5 sentences minimum) describing whether agent took action (added items) before asking needed clarification. Include turn numbers and sequence of events with specific quotes.\"\n",
    "  }},\n",
    "  \"leading_vague_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −4>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (3-5 sentences minimum) evaluating quality of clarifying questions. Were they clear and helpful, or leading/vague? Include specific question examples and explain why they were good or problematic.\"\n",
    "  }},\n",
    "  \"repeated_questions_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −5>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (2-4 sentences minimum) checking if agent asked same questions repeatedly without progress. Include turn count and whether conversation was efficient.\"\n",
    "  }},\n",
    "  \"didnt_repeat_back_penalty\": {{\n",
    "    \"applicable\": <true/false>,\n",
    "    \"points\": <0 or −3>,\n",
    "    \"reasoning\": \"EXTENSIVE explanation (2-4 sentences minimum) checking if agent confirmed understanding by repeating back user's requests. Include examples where agent did or didn't confirm.\"\n",
    "  }},\n",
    "  \"total_points\": <sum of all points (CAN BE NEGATIVE)>,\n",
    "  \"max_points\": 6,\n",
    "  \"penalties_applied\": [\"List each penalty with specific turn and quote\", \"e.g., −5 pts: Didn't ask about milk type in Turn 2 when user said 'add milk' (ambiguous)\"],\n",
    "  \"overall_reasoning\": \"COMPREHENSIVE SUMMARY (4-6 sentences minimum): If no ambiguity existed, explain why full 6/6 was awarded. If ambiguity existed, provide detailed explanation of how agent handled (or failed to handle) ambiguous situations with specific examples, turn numbers, and quotes from conversation. Total: [X/3] Asked When Needed + [X/2] References Context + [X/1] Resolved + [X] Penalties = X/6 pts\"\n",
    "}}\n",
    "\n",
    "CRITICAL:\n",
    "- If NO ambiguity, award 6/6 automatically and mark all positive criteria as N/A\n",
    "- If ambiguity exists, evaluate ALL 8 criteria with EXTENSIVE detailed reasoning\n",
    "- EVERY reasoning field MUST be 2-5 sentences with specific quotes, turn numbers, and analysis\n",
    "- Points CAN BE NEGATIVE if multiple penalties apply\n",
    "- Show work: explain WHY each criterion was awarded/penalized/N/A\n",
    "\"\"\"\n",
    "        result = self._call_openai_with_retry(prompt)\n",
    "        \n",
    "        # Convert to int in case LLM returns strings\n",
    "        def safe_int(val):\n",
    "            if isinstance(val, (int, float)):\n",
    "                return int(val)\n",
    "            try:\n",
    "                return int(val)\n",
    "            except (ValueError, TypeError):\n",
    "                return 0\n",
    "        \n",
    "        # Calculate points based on sub-criteria\n",
    "        points_earned = 0\n",
    "        max_points = 6  # Always 6 for clarifying questions\n",
    "        detailed_breakdown = []\n",
    "        \n",
    "        criterion_names = {\n",
    "            'asked_when_needed': ('Asked When Needed', 3),\n",
    "            'references_context': ('References Context', 2),\n",
    "            'resolved_quickly': ('Resolved Quickly', 1),\n",
    "            'didnt_ask_penalty': ('Didn\\'t Ask (Penalty)', 0),\n",
    "            'asked_after_acting_penalty': ('Asked After Acting (Penalty)', 0),\n",
    "            'leading_vague_penalty': ('Leading/Vague Questions (Penalty)', 0),\n",
    "            'repeated_questions_penalty': ('Repeated Questions (Penalty)', 0),\n",
    "            'didnt_repeat_back_penalty': ('Didn\\'t Repeat Back (Penalty)', 0)\n",
    "        }\n",
    "        \n",
    "        has_ambiguity = result.get('has_ambiguity', False)\n",
    "        \n",
    "        # If no ambiguity, award full points and mark positive criteria as N/A\n",
    "        if not has_ambiguity:\n",
    "            points_earned = 6\n",
    "            detailed_breakdown.append(\"✓ NO AMBIGUITY DETECTED - Full 6/6 points awarded automatically\\\\n  User requests were completely specific with no ambiguity requiring clarification.\")\n",
    "        else:\n",
    "            # Process all criteria\n",
    "            for criterion, (name, max_pts) in criterion_names.items():\n",
    "                if criterion in result and result[criterion].get('applicable', False):\n",
    "                    pts = safe_int(result[criterion].get('points', 0))\n",
    "                    reasoning = result[criterion].get('reasoning', 'No reasoning provided')\n",
    "                    points_earned += pts\n",
    "                    \n",
    "                    if 'penalty' in criterion.lower():\n",
    "                        detailed_breakdown.append(f\"✓ {name}: {pts} pts\\\\n  {reasoning}\")\n",
    "                    else:\n",
    "                        detailed_breakdown.append(f\"✓ {name}: {pts}/{max_pts} pts\\\\n  {reasoning}\")\n",
    "                else:\n",
    "                    reasoning = result.get(criterion, {}).get('reasoning', 'Not applicable')\n",
    "                    detailed_breakdown.append(f\"○ {name}: N/A\\\\n  {reasoning}\")\n",
    "        \n",
    "        penalties = result.get('penalties_applied', [])\n",
    "        penalties_text = \"\\\\n\\\\nPENALTIES APPLIED:\\\\n\" + \"\\\\n\".join(f\"  • {p}\" for p in penalties) if penalties else \"\"\n",
    "        \n",
    "        result['points_earned'] = points_earned\n",
    "        result['max_points'] = max_points\n",
    "        result['applicable'] = True  # Always applicable\n",
    "        result['reasoning'] = \"\\\\n\\\\n\".join(detailed_breakdown) + penalties_text + f\"\\\\n\\\\nTOTAL: {points_earned}/{max_points} points\"\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✓ ConvertedTraceGrader class defined (10-axis NOMI rubric)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a395884d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.891942Z",
     "iopub.status.busy": "2025-10-24T07:15:57.891847Z",
     "iopub.status.idle": "2025-10-24T07:15:57.894049Z",
     "shell.execute_reply": "2025-10-24T07:15:57.893873Z"
    },
    "papermill": {
     "duration": 0.004133,
     "end_time": "2025-10-24T07:15:57.894599",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.890466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Diagnostic: Check the structure of tool_calls in problematic trace\n",
    "test_trace_id = '509af828-e13f-4229-95ea-97a5f753cf12'\n",
    "test_trace = next((t for t in traces if t.get('task_id') == test_trace_id), None)\n",
    "\n",
    "if test_trace:\n",
    "    print(f\"Trace {test_trace_id}:\")\n",
    "    print(f\"Number of turns: {len(test_trace.get('turns', []))}\")\n",
    "    \n",
    "    for i, turn in enumerate(test_trace.get('turns', [])):\n",
    "        tool_calls = turn.get('tool_calls', [])\n",
    "        if tool_calls:\n",
    "            print(f\"\\nTurn {i}: Found tool_calls\")\n",
    "            print(f\"  Type: {type(tool_calls)}\")\n",
    "            print(f\"  Length: {len(tool_calls) if isinstance(tool_calls, list) else 'N/A'}\")\n",
    "            \n",
    "            if isinstance(tool_calls, list) and len(tool_calls) > 0:\n",
    "                print(f\"  First item type: {type(tool_calls[0])}\")\n",
    "                print(f\"  First item: {tool_calls[0][:200] if isinstance(tool_calls[0], str) else tool_calls[0]}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd5b374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.897263Z",
     "iopub.status.busy": "2025-10-24T07:15:57.897178Z",
     "iopub.status.idle": "2025-10-24T07:15:57.899362Z",
     "shell.execute_reply": "2025-10-24T07:15:57.899149Z"
    },
    "papermill": {
     "duration": 0.004153,
     "end_time": "2025-10-24T07:15:57.899908",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.895755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TEST: Grade only 4 specific traces IN PARALLEL\n",
    "\n",
    "# grader = ConvertedTraceGrader(openai_client)\n",
    "# print(\"✓ Grader instance created\")\n",
    "\n",
    "# def grade_single_trace(trace, index, total):\n",
    "#     \"\"\"Grade a single trace and return results\"\"\"\n",
    "#     task_id = trace.get('task_id', 'unknown')\n",
    "#     user_has_prefs = bool(trace.get('user_profile'))\n",
    "    \n",
    "#     print(f\"[{index}/{total}] Grading {task_id} (Prefs: {'✓' if user_has_prefs else '✗'})\")\n",
    "    \n",
    "#     grades = grader.grade_conversation(trace)\n",
    "    \n",
    "#     overall = grades.get('overall', {})\n",
    "#     points = overall.get('points_earned', 0)\n",
    "#     applicable = overall.get('applicable_points', 0)\n",
    "    \n",
    "#     print(f\"  → {points}/{applicable} pts\\n\")\n",
    "    \n",
    "#     return {\n",
    "#         'task_id': task_id,\n",
    "#         'grades': grades\n",
    "#     }\n",
    "\n",
    "# TEST_TRACE_IDS = [\n",
    "#     '06a51d53-484e-43a9-bcbb-d0f7187fbe92',\n",
    "#     '0476f1e1-b170-412c-ba0f-8e1b30dadb4f',\n",
    "#     '509af828-e13f-4229-95ea-97a5f753cf12',\n",
    "#     '7c32056f-bc50-4844-9b5f-2391a3e50610'\n",
    "# ]\n",
    "\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(f\"TEST GRADING: {len(TEST_TRACE_IDS)} traces (parallel)\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# # Filter to only test traces\n",
    "# test_traces = [t for t in traces if t.get('task_id') in TEST_TRACE_IDS]\n",
    "# print(f\"Found {len(test_traces)}/{len(TEST_TRACE_IDS)} test traces\\n\")\n",
    "\n",
    "# # Grade them in parallel\n",
    "# test_results = []\n",
    "# with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     future_to_trace = {\n",
    "#         executor.submit(grade_single_trace, trace, i+1, len(test_traces)): trace \n",
    "#         for i, trace in enumerate(test_traces)\n",
    "#     }\n",
    "    \n",
    "#     for future in as_completed(future_to_trace):\n",
    "#         result = future.result()\n",
    "#         test_results.append(result)\n",
    "\n",
    "# # Sort results by task_id to maintain order\n",
    "# test_results.sort(key=lambda x: TEST_TRACE_IDS.index(x['task_id']))\n",
    "\n",
    "# # Summary\n",
    "# print(f\"\\n{'='*80}\")\n",
    "# print(\"TEST GRADING SUMMARY\")\n",
    "# print(f\"{'='*80}\\n\")\n",
    "\n",
    "# for result in test_results:\n",
    "#     task_id = result['task_id']\n",
    "#     grades = result.get('grades', {})\n",
    "#     overall = grades.get('overall', {})\n",
    "    \n",
    "#     points = overall.get('points_earned', 0)\n",
    "#     applicable = overall.get('applicable_points', 0)\n",
    "    \n",
    "#     print(f\"\\n{task_id}:\")\n",
    "#     print(f\"  Overall: {points}/{applicable} pts\")\n",
    "    \n",
    "#     # Show breakdown\n",
    "#     for key in grader.max_points.keys():\n",
    "#         if key in grades:\n",
    "#             grade = grades[key]\n",
    "#             pts = grade.get('points_earned', 0)\n",
    "#             max_pts = grade.get('max_points', 0)\n",
    "#             is_applicable = grade.get('applicable', True)\n",
    "            \n",
    "#             category = key.replace('_', ' ').title()\n",
    "#             if is_applicable:\n",
    "#                 print(f\"    {category}: {pts}/{max_pts} pts\")\n",
    "#             else:\n",
    "#                 print(f\"    {category}: N/A\")\n",
    "\n",
    "# print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3555f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:15:57.902708Z",
     "iopub.status.busy": "2025-10-24T07:15:57.902627Z",
     "iopub.status.idle": "2025-10-24T07:18:19.115214Z",
     "shell.execute_reply": "2025-10-24T07:18:19.114712Z"
    },
    "papermill": {
     "duration": 141.215209,
     "end_time": "2025-10-24T07:18:19.116429",
     "exception": false,
     "start_time": "2025-10-24T07:15:57.901220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Grading 26 traces...\n",
      "================================================================================\n",
      "\n",
      "[1/26] Grading 129e7e81-9797-4653-8b04-8189fa1c4b91 (Consumer: 13465056, Prefs: ✓)...\n",
      "[2/26] Grading 1a0b7693-f81e-4492-9e73-4ceb9891828a (Consumer: 13465056, Prefs: ✓)...\n",
      "[3/26] Grading 3d050158-1e0e-4e4d-a652-a85ec8a6dd75 (Consumer: 1628566001, Prefs: ✓)...\n",
      "[4/26] Grading 41b59395-c547-4306-bcc4-08f64d1b584a (Consumer: 1628566001, Prefs: ✓)...\n",
      "[5/26] Grading 4a23a809-e2e5-4037-9e63-1fda10b43914 (Consumer: 917629603, Prefs: ✓)...\n",
      "[6/26] Grading 503d5d77-739b-4f0d-b8cb-670111aab533 (Consumer: 1628566001, Prefs: ✓)...\n",
      "[7/26] Grading 514521c7-3990-4643-ab3d-8d3357d08285 (Consumer: 10153581, Prefs: ✓)...\n",
      "[8/26] Grading 5bc06ca6-bf52-457a-9df8-b7451f8e269d (Consumer: 1620052939, Prefs: ✓)...\n",
      "[9/26] Grading 6cb8cc8c-f43d-4b32-b61f-0c1486675d03 (Consumer: 13465056, Prefs: ✓)...\n",
      "[10/26] Grading 7ebc7ef8-ab2c-46f9-ae42-b9ec88fb7d07 (Consumer: 1628566001, Prefs: ✓)...\n",
      "[11/26] Grading 853cf9e0-0923-4b6a-9cc0-1b1b5e478237 (Consumer: 1102045805, Prefs: ✓)...\n",
      "[12/26] Grading 86c7fd63-ddf8-433b-b8cb-2bc1afaab33a (Consumer: 1, Prefs: ✓)...\n",
      "[13/26] Grading 882d1d67-16af-418c-b7fd-0cf909c7d0a2 (Consumer: 1102045805, Prefs: ✓)...\n",
      "[14/26] Grading 8fbbb4d0-49de-43b6-818c-349264f14436 (Consumer: 2088552969, Prefs: ✓)...\n",
      "[15/26] Grading 90b6c194-5871-4d2f-b7c0-b2e2186b70c3 (Consumer: 1628566001, Prefs: ✓)...\n",
      "[16/26] Grading a0b4583c-1dde-4a54-b660-ec9a17c08be7 (Consumer: 1620052939, Prefs: ✓)...\n",
      "[17/26] Grading acbf4c48-9ee8-4423-b9b3-dff733a2d59a (Consumer: 1628566001, Prefs: ✓)...\n",
      "[18/26] Grading b15ce9c9-8d48-48f5-9db8-8c50f8f4c76a (Consumer: 169248016, Prefs: ✓)...\n",
      "[19/26] Grading b1823da2-86f2-43f9-992e-349a678d2763 (Consumer: 2010940165, Prefs: ✓)...\n",
      "[20/26] Grading cf17ffef-e72a-415c-972c-e47e7ccfbb1d (Consumer: 1102045805, Prefs: ✓)...\n",
      "[21/26] Grading d3aef5d3-5e31-41ab-a4b7-71f8250fd80c (Consumer: 1628566001, Prefs: ✓)...\n",
      "[22/26] Grading d5b76264-e6ce-452f-8764-d1d6c2f723c1 (Consumer: 1620052939, Prefs: ✓)...\n",
      "[23/26] Grading d952617c-5af9-436b-9049-14121cf7ded1 (Consumer: 1628566001, Prefs: ✓)...\n",
      "[24/26] Grading dc96d691-dbed-4e51-980e-b0f2f9d9ea98 (Consumer: 1620052939, Prefs: ✓)...\n",
      "[25/26] Grading eeb80165-772e-4b4e-a95d-0dd4ba8c60f1 (Consumer: 2010940165, Prefs: ✓)...\n",
      "[26/26] Grading fb2bacc9-d832-42e5-b347-469388300d30 (Consumer: 1628566001, Prefs: ✓)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 16/48 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 21/38 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 25/52 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - -5/38 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 4/53 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 75/86 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 48/73 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 41/48 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 15/58 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 71/80 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 35/57 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 71/71 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 72/90 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - -9/81 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 65/79 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 50/88 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 65/81 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 27/90 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 71/86 pts\n",
      "  ✓ PASS - 67/86 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 71/71 pts\n",
      "  ✓ PASS - 55/81 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 56/79 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 58/86 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ PASS - 65/86 pts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✗ FAIL - 54/92 pts\n",
      "\n",
      "✓ Grading complete!\n",
      "  Time: 141.2s\n",
      "  Avg per trace: 5.4s\n"
     ]
    }
   ],
   "source": [
    "# Grade all traces in parallel\n",
    "grader = ConvertedTraceGrader(openai_client)\n",
    "\n",
    "def grade_single_trace(trace: Dict, index: int, total: int) -> Dict:\n",
    "    \"\"\"Grade a single trace\"\"\"\n",
    "    task_id = trace.get('task_id', f'trace_{index}')\n",
    "    consumer_id = trace.get('consumer_id', 'unknown')\n",
    "    has_prefs = bool(trace.get('user_profile'))\n",
    "    print(f\"[{index}/{total}] Grading {task_id} (Consumer: {consumer_id}, Prefs: {'✓' if has_prefs else '✗'})...\")\n",
    "    \n",
    "    try:\n",
    "        grades = grader.grade_conversation(trace)\n",
    "        \n",
    "        points_earned = grades['overall']['points_earned']\n",
    "        applicable_points = grades['overall']['applicable_points']\n",
    "        status = \"✓ PASS\" if grades['overall']['passed'] else \"✗ FAIL\"\n",
    "        print(f\"  {status} - {points_earned}/{applicable_points} pts\")\n",
    "        \n",
    "        # Get preference summary\n",
    "        pref_summary = \"No preferences\"\n",
    "        if has_prefs:\n",
    "            try:\n",
    "                user_profile = trace.get('user_profile', {})\n",
    "                if user_profile and isinstance(user_profile, dict):\n",
    "                    dietary = user_profile.get('dietary_preference', {})\n",
    "                    if dietary and isinstance(dietary, dict):\n",
    "                        narrative = dietary.get('narrative', {})\n",
    "                        if narrative and isinstance(narrative, dict):\n",
    "                            statement = narrative.get('statement', '')\n",
    "                            if statement and isinstance(statement, str):\n",
    "                                pref_summary = f\"Dietary: {statement[:100]}...\"\n",
    "                            else:\n",
    "                                pref_summary = \"Profile available\"\n",
    "                        else:\n",
    "                            pref_summary = \"Profile available\"\n",
    "                    else:\n",
    "                        pref_summary = \"Profile available\"\n",
    "            except:\n",
    "                pref_summary = \"Profile available (error reading)\"\n",
    "        \n",
    "        return {\n",
    "            'task_id': task_id,\n",
    "            'trace': trace,\n",
    "            'grades': grades,\n",
    "            'has_preferences': has_prefs,\n",
    "            'preference_summary': pref_summary\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error grading: {str(e)}\")\n",
    "        return {\n",
    "            'task_id': task_id,\n",
    "            'trace': trace,\n",
    "            'grades': {'error': str(e)}\n",
    "        }\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Grading {len(traces)} traces...\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "graded_results = []\n",
    "grading_start = time.time()\n",
    "\n",
    "# Grade in parallel\n",
    "with ThreadPoolExecutor(max_workers=MAX_PARALLEL_GRADES) as executor:\n",
    "    future_to_trace = {\n",
    "        executor.submit(grade_single_trace, trace, i+1, len(traces)): trace \n",
    "        for i, trace in enumerate(traces)\n",
    "    }\n",
    "    \n",
    "    for future in as_completed(future_to_trace):\n",
    "        result = future.result()\n",
    "        graded_results.append(result)\n",
    "\n",
    "grading_elapsed = time.time() - grading_start\n",
    "\n",
    "print(f\"\\n✓ Grading complete!\")\n",
    "print(f\"  Time: {grading_elapsed:.1f}s\")\n",
    "print(f\"  Avg per trace: {grading_elapsed/len(graded_results):.1f}s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddcced11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:18:19.126420Z",
     "iopub.status.busy": "2025-10-24T07:18:19.126173Z",
     "iopub.status.idle": "2025-10-24T07:18:19.133207Z",
     "shell.execute_reply": "2025-10-24T07:18:19.132889Z"
    },
    "papermill": {
     "duration": 0.012248,
     "end_time": "2025-10-24T07:18:19.134056",
     "exception": false,
     "start_time": "2025-10-24T07:18:19.121808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GRADING SUMMARY (Points-Based)\n",
      "================================================================================\n",
      "\n",
      "Total traces: 26\n",
      "Valid grades: 26\n",
      "With preferences: 26/26 (100.0%)\n",
      "Errors: 0\n",
      "\n",
      "Pass Rate: 14/26 (53.8%)\n",
      "Average Score: 60.2%\n",
      "\n",
      "Average Scores by Category:\n",
      "--------------------------------------------------------------------------------\n",
      "Safety & Compliance                 (max 13):  4.81 pts ( 37.0%) [26/26 applicable]\n",
      "Store Selection                     (max 10):  7.83 pts ( 78.3%) [23/26 applicable]\n",
      "Search Quality                      (max 20): 12.22 pts ( 61.1%) [18/26 applicable]\n",
      "Pick Accuracy                       (max  8):  1.33 pts ( 16.7%) [18/26 applicable]\n",
      "Shopping List Building              (max 17):  8.19 pts ( 48.2%) [26/26 applicable]\n",
      "Apply to Cart                       (max  7):  7.00 pts (100.0%) [9/26 applicable]\n",
      "Personalization & Tone              (max 11):  2.54 pts ( 23.1%) [26/26 applicable]\n",
      "Reliability                         (max  3):  3.00 pts (100.0%) [26/26 applicable]\n",
      "Goal Completion                     (max  5):  3.46 pts ( 69.2%) [26/26 applicable]\n",
      "Clarifying Questions                (max  6):  4.81 pts ( 80.1%) [26/26 applicable]\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate summary\n",
    "valid_results = [r for r in graded_results if 'error' not in r.get('grades', {})]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"GRADING SUMMARY (Points-Based)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "with_prefs = sum(1 for r in graded_results if r.get('has_preferences', False))\n",
    "\n",
    "print(f\"Total traces: {len(graded_results)}\")\n",
    "print(f\"Valid grades: {len(valid_results)}\")\n",
    "print(f\"With preferences: {with_prefs}/{len(graded_results)} ({with_prefs/len(graded_results)*100:.1f}%)\")\n",
    "print(f\"Errors: {len(graded_results) - len(valid_results)}\\n\")\n",
    "\n",
    "if valid_results:\n",
    "    passed = sum(1 for r in valid_results if r['grades']['overall']['passed'])\n",
    "    \n",
    "    # Calculate average percentage from points\n",
    "    total_percentage = 0\n",
    "    for r in valid_results:\n",
    "        points_earned = r['grades']['overall']['points_earned']\n",
    "        applicable_points = r['grades']['overall']['applicable_points']\n",
    "        if applicable_points > 0:\n",
    "            total_percentage += (points_earned / applicable_points) * 100\n",
    "    avg_score = total_percentage / len(valid_results)\n",
    "    \n",
    "    print(f\"Pass Rate: {passed}/{len(valid_results)} ({passed/len(valid_results)*100:.1f}%)\")\n",
    "    print(f\"Average Score: {avg_score:.1f}%\\n\")\n",
    "    \n",
    "    print(\"Average Scores by Category:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Updated to use 10-axis rubric\n",
    "    categories = [\n",
    "        ('safety_compliance', 'Safety & Compliance', 13),\n",
    "        ('store_selection', 'Store Selection', 10),\n",
    "        ('search_quality', 'Search Quality', 20),\n",
    "        ('pick_accuracy', 'Pick Accuracy', 8),\n",
    "        ('shopping_list_building', 'Shopping List Building', 17),\n",
    "        ('apply_to_cart', 'Apply to Cart', 7),\n",
    "        ('personalization_tone', 'Personalization & Tone', 11),\n",
    "        ('reliability', 'Reliability', 3),\n",
    "        ('goal_completion', 'Goal Completion', 5),\n",
    "        ('clarifying_questions', 'Clarifying Questions', 6)\n",
    "    ]\n",
    "    \n",
    "    for cat_key, cat_name, max_pts in categories:\n",
    "        # Calculate average points and percentage for each category\n",
    "        applicable_results = [r for r in valid_results if r['grades'].get(cat_key, {}).get('applicable', True)]\n",
    "        \n",
    "        if applicable_results:\n",
    "            total_pts = sum(r['grades'][cat_key].get('points_earned', 0) for r in applicable_results)\n",
    "            avg_pts = total_pts / len(applicable_results)\n",
    "            avg_pct = (avg_pts / max_pts) * 100 if max_pts > 0 else 0\n",
    "            \n",
    "            print(f\"{cat_name:35s} (max {max_pts:2d}): {avg_pts:5.2f} pts ({avg_pct:5.1f}%) [{len(applicable_results)}/{len(valid_results)} applicable]\")\n",
    "        else:\n",
    "            print(f\"{cat_name:35s} (max {max_pts:2d}): N/A\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef6c96a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T07:18:19.140523Z",
     "iopub.status.busy": "2025-10-24T07:18:19.140368Z",
     "iopub.status.idle": "2025-10-24T07:18:19.175312Z",
     "shell.execute_reply": "2025-10-24T07:18:19.175037Z"
    },
    "papermill": {
     "duration": 0.038895,
     "end_time": "2025-10-24T07:18:19.176059",
     "exception": false,
     "start_time": "2025-10-24T07:18:19.137164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving results to vercel-deploy/batches/2025_10_23/new_graded_results.json...\n",
      "✓ Saved 26 graded results\n",
      "\n",
      "📊 View results at: http://localhost:8001/index.html\n",
      "   Load the grading file: vercel-deploy/batches/2025_10_23/new_graded_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save graded results\n",
    "print(f\"\\nSaving results to {OUTPUT_FILE}...\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(graded_results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved {len(graded_results)} graded results\")\n",
    "print(f\"\\n📊 View results at: http://localhost:8001/index.html\")\n",
    "print(f\"   Load the grading file: {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 142.741407,
   "end_time": "2025-10-24T07:18:19.394755",
   "environment_variables": {},
   "exception": null,
   "input_path": "grade_converted_traces.ipynb",
   "output_path": "grade_converted_traces_output.ipynb",
   "parameters": {},
   "start_time": "2025-10-24T07:15:56.653348",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
